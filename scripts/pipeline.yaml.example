# DeepRank-Ab Pipeline Configuration
# Copy this file to pipeline.yaml and edit for your run

# Required paths
run_root: /path/to/run_001           # Output directory (will be created)
pdb_root: /path/to/input_pdbs        # Input PDB directory
deeprank_root: /path/to/DeepRank-Ab  # DeepRank-Ab repository root
model_path: /path/to/model.pth       # Trained model weights

# Chain identifiers in input PDBs
chains:
  heavy: H
  light: "-"      # Use "-" for VHH (no light chain)
  antigen: T

# Dynamic resource allocation
# Array sizes are computed automatically based on input data
# These control max concurrent jobs to avoid overwhelming the cluster
max_concurrent_a: 20    # Max concurrent Stage A array jobs
max_concurrent_b: 10    # Max concurrent Stage B array jobs

# Stage A (CPU): Graph generation
stage_a:
  partition: norm
  cores: 32             # CPUs per task (auto-recommended based on input)
  mem_gb: 64
  time: "01:00:00"
  # Sharding parameters (controls how PDBs are grouped)
  target_shard_gb: 0.1  # Target shard size - smaller = more shards = more parallelism
  min_per_shard: 10     # Minimum PDBs per shard
  max_per_shard: 100    # Maximum PDBs per shard
  glob: "**/*.pdb"      # Glob pattern to find PDBs
  # Runtime/process controls
  split_mode: false               # true => submit Stage A as two arrays: prep_graphs then cluster_only
  strict_affinity: 0              # 1 => fail task if effective cpuset < requested cores
  use_srun_bind: 1                # 1 => launch Stage A payload via srun --cpu-bind
  srun_cpu_bind: cores            # passed to srun --cpu-bind
  graph_pipeline_telemetry: 0     # 1 => periodic graph enqueue/writer throughput logs
  graph_result_queue_maxsize: 100 # queue size between graph workers and writer
  graph_writer_log_every_s: 30    # telemetry log interval (seconds)
  anarci_parallel_batches: 1      # >1 => split ANARCI calls into parallel batches

# Stage B (GPU): ESM embeddings + inference
stage_b:
  partition: gpu
  gpus: 4               # GPUs per task
  cores: 32             # CPUs per task
  mem_gb: 128
  time: "04:00:00"
  # ESM settings
  esm_model: esm2_t33_650M_UR50D
  esm_toks_per_batch: 12288
  esm_scalar_dtype: float16
  # Inference settings
  batch_size: 64
  dl_workers: 8
  prefetch_factor: 4

# Stage C (CPU): Merge predictions
stage_c:
  partition: norm
  cores: 4
  mem_gb: 16
  time: "02:00:00"

# Optional settings
collect_metrics: true       # Enable compute metrics sampling
metrics_interval: 2         # Sampling interval in seconds
